{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ff045b",
   "metadata": {},
   "source": [
    "## 3.7 Fixpunktverfahren zum Lösen linearer Gleichungssysteme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33b8bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af309056",
   "metadata": {},
   "source": [
    "**Die einfache Richardson-Iteration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d4ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def richardson(A, b, x, it=1000, omega=1, tol=1e-5):\n",
    "    x = x.copy()\n",
    "    for i in range(it):\n",
    "        w = b - np.dot(A, x)\n",
    "        if np.linalg.norm(w) < tol:\n",
    "            break\n",
    "        x += omega * w\n",
    "    return x, i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a0549-e5fa-4e20-92d6-4677a6e59b9d",
   "metadata": {},
   "source": [
    "Wir testen die einfache Richardson-Iteration anhand des linearen Gleichungssystems $Ax=b$ mit\n",
    "$$A=\\begin{pmatrix}3 & 1.8 & 1\\\\ 1.4 & 2.3 & -0.7\\\\ 0.8 & 0.3 & 1.5 \\end{pmatrix}\\qquad\n",
    "b = \\begin{pmatrix} 1.2\\\\-2.1\\\\0.6\\end{pmatrix}.$$\n",
    "Mit `numpy` haben wir dabaei folgende 'exakte' Lösung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ed2ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[3.0, 1.8, 1],\n",
    "              [1.4, 2.3, -0.7],\n",
    "              [0.8, 0.3, 1.5]])\n",
    "b = np.array([1.2, -2.1, 0.6])\n",
    "\n",
    "x_np = np.linalg.solve(A, b)\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdca537-fa06-42ea-9675-c696fbf6ce45",
   "metadata": {},
   "source": [
    "Wir wenden nun die Richardson-Iteration auf dieses System mit Relaxationsparameter $\\omega=1$ an und nehmen den Startvektor $x_0 = (1, -1, 0)$, der sogar recht nahe an der Lösung ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f763e-3e8e-479a-a4fb-0808578ce78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1.0, -1.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de1154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, n = richardson(A, b, x0, it=100, omega=1)\n",
    "print(f'x = {x} nach {n} Schritten')\n",
    "print(f'||x - x_ex||_2 = {np.linalg.norm(x - x_np)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68417be3-ad5e-4236-a0e7-bfe7e2e51680",
   "metadata": {},
   "source": [
    "Leider scheint das Verfahren zu divergieren. Nehmen wir aber einen kleineren Relaxationsparameter, können wir Konvergenz erhalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9756d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, n = richardson(A, b, x0, it=50, omega=0.4)\n",
    "print(f'x = {x} nach {n} Schritten')\n",
    "print(f'||x - x_ex||_2 = {np.linalg.norm(x - x_np)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca779a",
   "metadata": {},
   "source": [
    "**Das Jacobi-Verfahren**\n",
    "\n",
    "Diese Implementierung ist für `numpy` **nicht optimal**, da wir z.B. die Matrix-Vektor-Produkte selber berechnen und nicht die hierfür von `numpy` zur Verfügung gestellten Routinen verwenden. Wir nutzen diese Implementierungen um die Unterschiede zwischen dem Jacobi- und Gauß-Seidel-Verfahren zu verdeutlichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f2e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobi(A, b, x, it=1000, tol=1e-5):\n",
    "    n, m = A.shape\n",
    "    x, x_neu = x.copy(), x.copy()\n",
    "    for k in range(it):\n",
    "        if np.linalg.norm(b - np.dot(A, x)) < tol:\n",
    "            break\n",
    "        for i in range(n):\n",
    "            s = 0\n",
    "            for j in range(i):\n",
    "                s += A[i, j] * x[j]\n",
    "            for j in range(i + 1, n):\n",
    "                s += A[i, j] * x[j]\n",
    "            x_neu[i] = (b[i] - s) / A[i, i]\n",
    "        x[:] = x_neu\n",
    "    return x, k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5353983c-5d06-4c12-95cf-7293af6778f6",
   "metadata": {},
   "source": [
    "Angewandt auf unser obiges Beispiel erhalten wir dann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eee4bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, n = jacobi(A, b, x0, it=50)\n",
    "print(f'x = {x} nach {n} Schritten')\n",
    "print(f'||x - x_ex||_2 = {np.linalg.norm(x - x_np)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b2700b",
   "metadata": {},
   "source": [
    "**Das Gauß-Seidel Verfahren**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_seidel(A, b, x, it=100, tol=1e-5):\n",
    "    n, m = A.shape\n",
    "    x, x_neu = x.copy(), x.copy()\n",
    "    for k in range(it):\n",
    "        if np.linalg.norm(b - np.dot(A, x)) < tol:\n",
    "            break\n",
    "        for i in range(n):\n",
    "            s = 0\n",
    "            for j in range(i):\n",
    "                s += A[i, j] * x_neu[j]\n",
    "            for j in range(i + 1, n):\n",
    "                s += A[i, j] * x[j]\n",
    "            x_neu[i] = (b[i] - s) / A[i, i]\n",
    "        x[:] = x_neu\n",
    "    return x, k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb8ce33-04ae-4d0f-a268-3db526f91274",
   "metadata": {},
   "source": [
    "Angewandt auf unser obiges Beispiel erhalten wir dann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05fe6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, n = gauss_seidel(A, b, x0, it=50)\n",
    "print(f'x = {x} nach {n} Schritten')\n",
    "print(f'||x - x_ex||_2 = {np.linalg.norm(x - x_np)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3b1e11-51c4-44cf-9d9e-a5568e682b2a",
   "metadata": {},
   "source": [
    "Das Gauß-Seidel-Verfahren benötigt also in etwa nur die Hälfte der Schritte wie das Jacobi-Verfahren.\n",
    "\n",
    "### 3.7.1 Konvergenzkriterium für Jacobi- und Gauß-Seidel-Iteration\n",
    "\n",
    "#### Beispiel 8.12 (Jacobi- und Gauß-Seidel-Verfahren bei der Modellmatrix)\n",
    "\n",
    "Wir betrachten das lineare Gleichungssystem $Ax=b$ mit der Modellmatrix $A\\in\\mathbb{R}^{n\\times n}$ \n",
    "$$ A = \\begin{pmatrix}2 & -1 \\\\ -1 & 2 & -1 \\\\ & \\ddots & \\ddots & \\ddots \\\\ && -1 & 2 & -1\\\\ &&& -1 & 2 \\end{pmatrix}$$\n",
    "sowie der rechten Seite $b\\in\\mathbb{R}^n$ mit $b=(1,\\dots,1)^T$. Die Matrix ist also irreduzibel, des Weiteren diagonaldominant und in erster und letzter Zeile auch stark diagonaldominant. Die Jacobi- und Gauß-Seidel-Verfahren konvergieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083ef586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f46ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    n = 10 * 2**i\n",
    "    A = np.diag(2 * np.ones(n), k=0) + np.diag(-1 * np.ones(n - 1), k=1) + np.diag(-1 * np.ones(n - 1), k=-1)\n",
    "    b = np.ones(n)\n",
    "    x0 = np.zeros(n)\n",
    "    \n",
    "    t = time.perf_counter()\n",
    "    x, m = jacobi(A, b, x0, it=int(2e6), tol=1e-4)\n",
    "    t = time.perf_counter() - t\n",
    "    \n",
    "    res = np.linalg.norm(b - np.dot(A, x))\n",
    "    print(f'Jacobi: n = {n:03d}, Schritte = {m:07d} time = {t:07.3f}sec, res = {res:4.2e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e956fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    n = 10 * 2**i\n",
    "    A = np.diag(2 * np.ones(n), k=0) + np.diag(-1 * np.ones(n - 1), k=1) + np.diag(-1 * np.ones(n - 1), k=-1)\n",
    "    b = np.ones(n)\n",
    "    x0 = np.zeros(n)\n",
    "    \n",
    "    t = time.perf_counter()\n",
    "    x, m = gauss_seidel(A, b, x0, it=int(2e6), tol=1e-4)\n",
    "    t = time.perf_counter() - t\n",
    "    \n",
    "    res = np.linalg.norm(b - np.dot(A, x))\n",
    "    print(f'Gauß-Seidel: n = {n:03d}, Schritte = {m:07d} time = {t:07.3f}sec, res = {res:4.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c3b5d4",
   "metadata": {},
   "source": [
    "Hier sehen wir, dass das Gauß-Seidel-Verfahren fast genau halb so viele Schritte wie das Jacobi-Verfahren benötigt und somit bei gleich effizienter Implementierung doppelt so schnell ist.\n",
    "\n",
    "Wir können beide Verfahren mit `numpy` auch effizienter implementieren, indem wir die vorhandenen Routinen für Skalarprodukte verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ccd190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobi_np(A, b, x, it=1000, tol=1e-5):\n",
    "    n, m = A.shape\n",
    "    d = np.diag(A)\n",
    "    x = x.copy()\n",
    "    for k in range(it):\n",
    "        res = b - A.dot(x)\n",
    "        if np.linalg.norm(res) < tol:\n",
    "            break\n",
    "        x += res / d\n",
    "    return x, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a0b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_seidel_np(A, b, x, it=100, tol=1e-5):\n",
    "    n, m = A.shape\n",
    "    x = x.copy()\n",
    "    for k in range(it):\n",
    "        if np.linalg.norm(b - np.dot(A, x)) < tol:\n",
    "            break\n",
    "        x_alt = x.copy()\n",
    "        for i in range(n):\n",
    "            s1 = np.dot(A[i, :i], x[:i])\n",
    "            s2 = np.dot(A[i, i + 1:], x_alt[i + 1:])\n",
    "            x[i] = (b[i] - s1 - s2) / A[i, i]\n",
    "    return x, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b09023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    n = 10 * 2**i\n",
    "    A = np.diag(2 * np.ones(n), k=0) + np.diag(-1 * np.ones(n - 1), k=1) + np.diag(-1 * np.ones(n - 1), k=-1)\n",
    "    b = np.ones(n)\n",
    "    x0 = np.zeros(n)\n",
    "    \n",
    "    t = time.perf_counter()\n",
    "    x, m = jacobi_np(A, b, x0, it=int(2e6), tol=1e-4)\n",
    "    t = time.perf_counter() - t\n",
    "    \n",
    "    res = np.linalg.norm(b - np.dot(A, x))\n",
    "    print(f'Jacobi: n = {n:03d}, Schritte = {m:07d} time = {t:07.3f}sec, res = {res:4.2e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4bb2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    n = 10 * 2**i\n",
    "    A = np.diag(2 * np.ones(n), k=0) + np.diag(-1 * np.ones(n - 1), k=1) + np.diag(-1 * np.ones(n - 1), k=-1)\n",
    "    b = np.ones(n)\n",
    "    x0 = np.zeros(n)\n",
    "    \n",
    "    t = time.perf_counter()\n",
    "    x, m = gauss_seidel_np(A, b, x0, it=int(1e6), tol=1e-4)\n",
    "    t = time.perf_counter() - t\n",
    "    \n",
    "    res = np.linalg.norm(b - np.dot(A, x))\n",
    "    print(f'Gauß-Seidel: n = {n:03d}, Schritte = {m:07d} time = {t:07.3f}sec, res = {res:4.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb640dc",
   "metadata": {},
   "source": [
    "Wir sehen also, dass der Aufwand auf die einzelnen Einträge von Python aus zuzugreifen, und diese zu verändern, deutlich teurer ist, als die interne Berechnung des Matrix-Vektor-Produktes. Damit ist jetzt das Jacobi-Verfahren schneller, obwohl doppelt so viele Schritte verwendet werden. In dem wir auf die Matrix-Form des Verfahrens zurückgreifen, und das Vorwärtseinsetzen aus `scipy` verwenden, können wir mit dem Gauß-Seidel eine vergleichbare Effizienz erreichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb789624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "def gauss_seidel_sp(A, b, x, it=100, tol=1e-5):\n",
    "    n, m = A.shape\n",
    "    x = x.copy()\n",
    "    R = np.triu(A, 1)\n",
    "    LD = np.tril(A, 0)\n",
    "    for k in range(it):\n",
    "        if np.linalg.norm(b - np.dot(A, x)) < tol:\n",
    "            break\n",
    "        x = sp.linalg.solve_triangular(LD, b - np.dot(R, x), lower=True)\n",
    "    return x, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b2854d-1423-4779-a641-e56cd739ec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    n = 10 * 2**i\n",
    "    A = np.diag(2 * np.ones(n), k=0) + np.diag(-1 * np.ones(n - 1), k=1) + np.diag(-1 * np.ones(n - 1), k=-1)\n",
    "    b = np.ones(n)\n",
    "    x0 = np.zeros(n)\n",
    "    \n",
    "    t = time.perf_counter()\n",
    "    x, m = gauss_seidel_sp(A, b, x0, it=int(1e6), tol=1e-4)\n",
    "    t = time.perf_counter() - t\n",
    "    \n",
    "    res = np.linalg.norm(b - np.dot(A, x))\n",
    "    print(f'Gauß-Seidel: n = {n:03d}, Schritte = {m:07d} time = {t:07.3f}sec, res = {res:4.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a1c5d6-1633-4a2c-8e9e-ba06401a9a94",
   "metadata": {},
   "source": [
    "Hier durch wird noch einmal deutlich, dass die effiziente Umsetzung eines Algorithmus eine zentrale Rolle spielt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e76ae1-c9eb-4104-b7e1-586cdc1adf78",
   "metadata": {},
   "source": [
    "### 3.7.2 Relaxationsverfahren: Das SOR-Verfahren\n",
    "\n",
    "Wir implementieren das SOR-Verfahren in der Index-Form, damit die Effizienz direkt mit den Index-Implementierungen der Jacobi- und Gauß-Seidel-Verfahren verglichen werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddbc250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sor(A, b, x, omega, it=100, tol=1e-5):\n",
    "    assert (omega > 0 and omega < 2), 'Omega nicht im Intervall (0, 2)'\n",
    "    n, m = A.shape\n",
    "    x, x_neu = x.copy(), x.copy()\n",
    "    for k in range(it):\n",
    "        if np.linalg.norm(b - np.dot(A, x)) < tol:\n",
    "            break\n",
    "        for i in range(n):\n",
    "            s = 0\n",
    "            for j in range(i):\n",
    "                s += A[i, j] * x_neu[j]\n",
    "            for j in range(i + 1, n):\n",
    "                s += A[i, j] * x[j]\n",
    "            x_neu[i] = omega * (b[i] - s) / A[i, i] + (1 - omega) * x[i]\n",
    "        x[:] = x_neu\n",
    "    return x, k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2dfac7-c83f-429c-b392-cdfa7d2952dc",
   "metadata": {},
   "source": [
    "Dazu betrachten wir zunächst wieder das lineare Gleichungssystem $Ax=b$ mit\n",
    "$$A=\\begin{pmatrix}3 & 1.8 & 1\\\\ 1.4 & 2.3 & -0.7\\\\ 0.8 & 0.3 & 1.5 \\end{pmatrix}\\qquad\n",
    "b = \\begin{pmatrix} 1.2\\\\-2.1\\\\0.6\\end{pmatrix}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e52bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[3.0, 1.8, 1],\n",
    "              [1.4, 2.3, -0.7],\n",
    "              [0.8, 0.3, 1.5]])\n",
    "b = np.array([1.2, -2.1, 0.6])\n",
    "\n",
    "x_np = np.linalg.solve(A, b)\n",
    "\n",
    "x0 = np.array([1.0, -1.0, 0.0])\n",
    "x, n = sor(A, b, x0, it=100, omega=1.2)\n",
    "print(f'x = {x} nach {n} Schritten')\n",
    "print(f'||x - x_ex||_2 = {np.linalg.norm(x - x_np)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f10b25-f943-4079-a5ec-4ed1891f39d8",
   "metadata": {},
   "source": [
    "Das Verfahren konvergiert also schneller als die bisherigen Verfahren. Dies hängt aber wesentlich von der korrekten Wahl von $\\omega$ ab. Probieren Sie bitte einmal andere Werte für `omega`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9cd96c-95f9-479a-96e3-e87a0cdfc2b7",
   "metadata": {},
   "source": [
    "#### Beispiel 8.14 (Modellmatrix mit SOR-Verfahren)\n",
    "\n",
    "Wir kehren wieder zur Modellmatrix zurück"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ced6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    n = 10 * 2**i\n",
    "    A = np.diag(2 * np.ones(n), k=0) + np.diag(-1 * np.ones(n - 1), k=1) + np.diag(-1 * np.ones(n - 1), k=-1)\n",
    "    b = np.ones(n)\n",
    "    x0 = np.zeros(n)\n",
    "    \n",
    "    lam = 1 - np.pi**2 / (2 * (n+1)**2)\n",
    "    omega = 2 * (1 - np.sqrt(1 - lam**2)) / lam**2\n",
    "    \n",
    "    t = time.perf_counter()\n",
    "    x, m = sor(A, b, x0, omega=omega, it=int(2e6), tol=1e-4)\n",
    "    t = time.perf_counter() - t\n",
    "    \n",
    "    res = np.linalg.norm(b - np.dot(A, x))\n",
    "    print(f'SOR: n = {n:03d}, Schritte = {m:07d} time = {t:07.3f}sec, res = {res:4.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73866fea",
   "metadata": {},
   "source": [
    "Bei $n=80$ ist das SOR also in etwa 40 mal schneller als das Gauß-Seidl-Verfahren.\n",
    "\n",
    "Wir können die Implementierung wieder mit `numpy` etwas verbessern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b060f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sor_np(A, b, x, omega, it=100, tol=1e-5):\n",
    "    n, m = A.shape\n",
    "    x, x_alt = x.copy(), x.copy()\n",
    "    for k in range(it):\n",
    "        if np.linalg.norm(b - np.dot(A, x)) < tol:\n",
    "            break\n",
    "        x_alt[:] = x\n",
    "        for i in range(n):\n",
    "            s1 = np.dot(A[i, :i], x[:i])\n",
    "            s2 = np.dot(A[i, i + 1:], x_alt[i + 1:])\n",
    "            x[i] = omega * (b[i] - s1 - s2) / A[i, i] + (1 - omega) * x_alt[i]\n",
    "    return x, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    n = 10 * 2**i\n",
    "    A = np.diag(2 * np.ones(n), k=0) + np.diag(-1 * np.ones(n - 1), k=1) + np.diag(-1 * np.ones(n - 1), k=-1)\n",
    "    b = np.ones(n)\n",
    "    x0 = np.zeros(n)\n",
    "    \n",
    "    lam = 1 - np.pi**2 / (2 * (n+1)**2)\n",
    "    omega = 2 * (1 - np.sqrt(1 - lam**2)) / lam**2\n",
    "    \n",
    "    t = time.perf_counter()\n",
    "    x, m = sor_np(A, b, x0, omega=omega, it=int(2e6), tol=1e-4)\n",
    "    t = time.perf_counter() - t\n",
    "    \n",
    "    res = np.linalg.norm(b - np.dot(A, x))\n",
    "    print(f'SOR: n = {n:03d}, Schritte = {m:07d} time = {t:07.3f}sec, res = {res:4.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5a171c",
   "metadata": {},
   "source": [
    "Durch die erhebliche Reduktion der Anzahl der notwendigen Schritte, ist das SOR-Verfahren sogar schneller als das Jacobi-Verfahren mit `numpy` Matrix-Vektor-Produkten, obwohl wir hier wieder auf einzelne Einträge zugreifen müssen. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
