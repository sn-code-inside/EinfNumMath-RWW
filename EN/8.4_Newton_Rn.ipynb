{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fbfb4b0",
   "metadata": {},
   "source": [
    "## 8.4 Solving Nonlinear Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6424af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scripts.lu import lu_pivot, forward, backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd689f4",
   "metadata": {},
   "source": [
    "**Implementation 8.45: Newtonâ€™s Method in $\\mathbb{R}^n$**\n",
    "\n",
    "To implement Newton's method in higher dimensions, we have to solve a linear system in every iteration. To this end, we use our implementation of the LU decomposition with pivoting,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f841a342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_vec(f, Df, x, n=10, tol=1e-10):\n",
    "    print('it  ||f(x)||')\n",
    "    print('--------------')\n",
    "    b, y, w = np.zeros_like(x), np.zeros_like(x), np.zeros_like(x)\n",
    "\n",
    "    for i in range(n):\n",
    "        b[:] = - f(x)\n",
    "        print(f'{i:02d} {np.linalg.norm(b): .4e}')\n",
    "        if np.linalg.norm(b) < tol:\n",
    "            break\n",
    "        \n",
    "        jac = Df(x)\n",
    "        pivot = lu_pivot(jac)\n",
    "        \n",
    "        for p in pivot:\n",
    "            b[p] = b[[p[1], p[0]]]\n",
    "        y[:] = forward(jac, b)\n",
    "        w[:] = backward(jac, y)\n",
    "        x[:] += w\n",
    "    else: \n",
    "        print(f\"Newton's method did not converge after {n} iterations.\", end=' ')\n",
    "        print(f'||res||={np.linalg.norm(f(x)):.4e}')\n",
    "    return i, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491e71cb-898a-49fb-b5d9-5eb048992fcb",
   "metadata": {},
   "source": [
    "#### Example 8.44 (Newton's Method in $\\mathbb{R}^n$)\n",
    "\n",
    "We search for the zeros of the function\n",
    "$$f(x_1,x_2) = \\begin{pmatrix}1-x_1^2-x_2^2 \\\\ (x_1-2x_2)/(1/2+x_2)\\end{pmatrix}$$\n",
    "with Jacobian\n",
    "$$ Df(x) = \\begin{pmatrix} -2x_1 & -2x_2 \\\\ \\frac{2}{1+2x_2} & -\\frac{4+4x_1}{(1+2x_2)^2}\\end{pmatrix}.$$\n",
    "The solution is given by\n",
    "$$x \\approx \\pm (0.894427191, 0.447213595).$$\n",
    "We start by implementing the function and the Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24d7443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x, y = x[:]\n",
    "    return np.array([1 - x**2 - y**2, (x - 2 * y) / (1 / 2 + y)], dtype=np.double)\n",
    "\n",
    "def Df(x):\n",
    "    x, y = x[:]\n",
    "    return np.array([[-2 * x, -2 * y],\n",
    "                     [2 / (1 + 2 * y), - (4 + 4 * x)/ (1 + 2 * y)**2]], dtype=np.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24aaa95-4a82-4dcb-afbb-5a7da0a498a6",
   "metadata": {},
   "source": [
    "Using the initial guess $x_0 = (1, 1)^T$, we obtain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db447cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, x = newton_vec(f, Df, x=np.array([1.0, 1.0]), n=15, tol=1e-10)\n",
    "print(f'x = {x} after {n} steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab868507-96ee-4c75-aaa9-2c23a70e7044",
   "metadata": {},
   "source": [
    "With the initial guess $x_0 = (-1, -0.2)^T$, we obtain the second solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705fe6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, x = newton_vec(f, Df, x=np.array([-1.0, -0.2]), n=15, tol=1e-10)\n",
    "print(f'x = {x} after {n} steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492db345-5976-4578-b379-6bbd7c615277",
   "metadata": {},
   "source": [
    "The most expensive step in Newton's method is the computation of the LU decomposition. Quasi-Newton methods can, therefore, be particularly useful.\n",
    "\n",
    "**Implementation 8.48: Simplified Newton method in $\\mathbb{R}^n$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717abe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_simpl_vec(f, Df, x, n=10, tol=1e-10):\n",
    "    b, y, w = np.zeros_like(x), np.zeros_like(x), np.zeros_like(x)\n",
    "    jac = Df(x)\n",
    "    pivot = lu_pivot(jac)\n",
    "    \n",
    "    for i in range(n):\n",
    "        b[:] = - f(x)\n",
    "        if np.linalg.norm(b) < tol:\n",
    "            break\n",
    "        for p in pivot:\n",
    "            b[p] = b[[p[1], p[0]]]\n",
    "        y[:] = forward(jac, b)\n",
    "        w[:] = backward(jac, y)\n",
    "        x[:] += w\n",
    "    else:\n",
    "        print(f'The simplified Newton method did not converge after {n} iterations.', end=' ')\n",
    "        print(f'||res||={np.linalg.norm(f(x)):.4e}')\n",
    "    return i, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ea62c7-6403-4ed1-a4bc-07ab29481388",
   "metadata": {},
   "source": [
    "We apply the simplified Newton method to the same example. We can see that the choice of the starting solution (i.e. where we invert the Jacobian) has a particularly large influence on the convergence of the scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, x = newton_simpl_vec(f, Df, x=np.array([1.0, 0.5]), n=50, tol=1e-10)\n",
    "print(f'x = {x} after {n} steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cbf428",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, x = newton_simpl_vec(f, Df, x=np.array([1.0, 1.0]), n=700, tol=1e-10)\n",
    "print(f'x = {x} after {n} steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65296f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, x = newton_simpl_vec(f, Df, x=np.array([-1.0, 1.0]), n=1000, tol=1e-10)\n",
    "print(f'x = {x} after {n} steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4557608c-d571-45f5-bbeb-c32b2f7704ff",
   "metadata": {},
   "source": [
    "As you can see, the method does not necessarily converge.\n",
    "\n",
    "Compare the runtime of the simplified Newton method using jupyter cell-magic `%%timeit` with $x_0 = (1, 0.5)$ with Newton's method and the initial value $x_0 = (1,1)$. What do you observe? What do you conclude about the computational effort of the individual iteration steps of the two methods? What do you expect if the dimension of the problem increases?\n",
    "\n",
    "### 8.4.2 Globalization of Newton's method\n",
    "\n",
    "**Example 8.50 (The Damped Newton Method)**\n",
    "\n",
    "As we have seen, the simplified Newton method only converges sometimes. However, it can be advantageous because each individual step is significantly faster. However, Newton's method also does not converge if the initial guess is too far away from the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ea9fa-5922-4556-9789-a3dcfd04d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "newton_vec(f, Df, x=np.array([0, -0.49999]), n=35, tol=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e097cd-e5e4-467d-bd00-63268f0a1fbc",
   "metadata": {},
   "source": [
    "To increase the area of convergence, we may damp each iteration step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1849f64f-2c8e-4861-a940-cd1a51b59336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_damped_vec(f, Df, x, omega, n=10, tol=1e-10):\n",
    "    assert len(omega) == n, 'Number of damping parameters must match number of steps'\n",
    "    b, y, w = np.zeros_like(x), np.zeros_like(x), np.zeros_like(x)\n",
    "\n",
    "    print('it  ||f(x)||')\n",
    "    print('--------------')\n",
    "    \n",
    "    for i in range(n):\n",
    "        b[:] = - f(x)\n",
    "        print(f'{i:02d} {np.linalg.norm(b): .4e}')\n",
    "        if np.linalg.norm(b) < tol:\n",
    "            break\n",
    "        jac = Df(x)\n",
    "        pivot = lu_pivot(jac)\n",
    "        for p in pivot:\n",
    "            b[p] = b[[p[1], p[0]]]\n",
    "        y[:] = forward(jac, b)\n",
    "        w[:] = backward(jac, y)\n",
    "        x[:] += omega[i] * w\n",
    "    else: \n",
    "        print(f'The damped Newton method did not converge after {n} iterations.', end=' ')\n",
    "        print(f'||res||={np.linalg.norm(f(x)):.4e}')\n",
    "    return i, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c462807-7b85-4816-95ad-425c03d5c55a",
   "metadata": {},
   "source": [
    "Using the constant damping parameter $\\omega=0.88$ then leads to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d59f6-9487-4c12-b484-e943f28ccd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "newton_damped_vec(f, Df, x=np.array([0, -0.49999]), omega=[0.88] * 50, n=50, tol=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e22aab4-7424-4d97-9013-ead0e2d0aa53",
   "metadata": {},
   "source": [
    "Even the simplified Newton method may be improved with the correct choice of damping parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1c55f-687e-49e8-bf34-88181af74be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_simpl_damped_vec(f, Df, x, omega, n=10, tol=1e-10):\n",
    "    assert len(omega) == n, 'Number of damping parameters must match number of steps'\n",
    "    b, y, w = np.zeros_like(x), np.zeros_like(x), np.zeros_like(x)\n",
    "    jac = Df(x)\n",
    "    pivot = lu_pivot(jac)\n",
    "    for i in range(n):\n",
    "        b[:] = - f(x)\n",
    "        if np.linalg.norm(b) < tol:\n",
    "            break\n",
    "        for p in pivot:\n",
    "            b[p] = b[[p[1], p[0]]]\n",
    "        y[:] = forward(jac, b)\n",
    "        w[:] = backward(jac, y)\n",
    "        x[:] += omega[i] * w\n",
    "    else: \n",
    "        print(f'The damped simplified Newton method did not converge after {n} iterations.', end=' ')\n",
    "        print(f'||res||={np.linalg.norm(f(x)):.4e}')\n",
    "    return i, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960d08dd-8d38-4b51-912f-ec3764e1c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "newton_simpl_damped_vec(f, Df, x=np.array([1.0, 1.0]), omega=[0.74] * 30, n=30, tol=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b8fe2f-08ae-4fd3-af0f-ccc25dd03852",
   "metadata": {},
   "source": [
    "Only 27 steps are needed in comparison to the previous 670 steps.\n",
    "\n",
    "As we do not go as far in the wrong direction with the initial steps, the method converges faster than Newton's method. However, we have slow convergence in each step. It is, therefore, important to switch back to Newton's method once the approximate solution is in the area of the quadratic convergence.\n",
    "\n",
    "**Implementation 8.55: Globalized Newton method**\n",
    "\n",
    "We have seen that the simplified Newton method requires 670 steps with the initial guess $x_0 = (1,1)$. With an appropriate choice of damping parameter, we were able to reduce this to 27 steps. However, the correct choice of $\\omega$ is not easy. In order to increase the area of convergence in the vector-valued simplified Newton method, we can also incorporate a line search. This allows the largest possible damping parameter to be selected. To make the method more robust, the Jacobian is only updated if the convergence is poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7750c2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_global_vec(f, D, x, sigma=0.5, Lmax=10, n=10, tol=1e-10):\n",
    "    x0, b, y, w = [np.zeros_like(x) for _ in range(4)]\n",
    "    \n",
    "    b[:] = - f(x)\n",
    "    res0, res1 = np.linalg.norm(b), float('nan')\n",
    "    if res0 < tol:\n",
    "        return 0, x\n",
    "\n",
    "    for i in range(n):\n",
    "        if i == 0 or res0 / res1 > 0.3:\n",
    "            print(f'i = {i}: Jacobian updated')\n",
    "            jac = D(x)\n",
    "            pivot = lu_pivot(jac)\n",
    "        for p in pivot:\n",
    "            b[p] = b[[p[1], p[0]]]\n",
    "        y[:] = forward(jac, b)\n",
    "        w[:] = backward(jac, y)\n",
    "        \n",
    "        for l in range(Lmax):\n",
    "            x0[:] = x + sigma**l * w\n",
    "            b[:] = -f(x0)\n",
    "            res2 = np.linalg.norm(b)\n",
    "            if res2 < res0:\n",
    "                if l > 0:\n",
    "                    print(f'i = {i}: {l} line-search steps necessary')\n",
    "                x[:] = x0\n",
    "                break\n",
    "        else:\n",
    "            print(f'Step {i}: {l} line search steps not sufficient, ||res|| = {res2:.4e}')\n",
    "            x[:] = x0\n",
    "        \n",
    "        res1 = res0\n",
    "        res0 = res2\n",
    "        if res0 < tol:\n",
    "            break\n",
    "    else:\n",
    "        print(f'The globalized Newton method did not converge after {n} iterations.', end=' ')\n",
    "        print(f'||res|| = {res0:.4e}')\n",
    "    return i, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa7ea9e-4486-4f3c-ac93-8cd8ef1f5ea1",
   "metadata": {},
   "source": [
    "Applying this with the initial guess $x_0 = (1,1)^T$, where we have had 
     670 steps, then we see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece135f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, x = newton_global_vec(f, Df, x=np.array([1.0, 1.0]), n=20, tol=1e-10)\n",
    "print(f'\\nn = {n}, x = {x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fca187-1a7f-4c67-9d21-9e34865412ce",
   "metadata": {},
   "source": [
    "By just updating the Jacobian once, we require less than 40 times fewer steps compared with the simplified Newton Method. Let's try the initial guess $x_0=(-1, 1)^T$, with which the simplified Newton method previously did not converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9af7008",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, x = newton_global_vec(f, Df, x=np.array([-1.0, 1.0]), n=50, tol=1e-10)\n",
    "print(f'\\nn = {n}, x = {x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbefd4e0-3848-45bf-bb13-6706e5ce84e4",
   "metadata": {},
   "source": [
    "By combing the Jacobian update with a line search, we are able to reach the area of convergence. Taking the initial guess $x_0=(0, -0.49999)$ where the Jacobian is nearly singular, we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0997521b-fbc2-4a33-976e-c590bc7eca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, x = newton_global_vec(f, Df, x=np.array([0, -0.49999]), n=50, tol=1e-10)\n",
    "print(f'\\nn = {n}, x = {x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a1e910-439f-4fa2-bd7a-d9a0ebb8a2ab",
   "metadata": {},
   "source": [
    "We again get convergence of the scheme. However, we need more updates of the Jacobian and more line search steps before we reach the area of fast convergence where we can reuse the same Jacobian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f591d420",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
