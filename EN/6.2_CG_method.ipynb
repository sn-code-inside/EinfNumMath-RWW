{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62bfe673",
   "metadata": {},
   "source": [
    "## 6.2 The Conjugate Gradient Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a5f25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871ac0f2-21f3-4c44-8978-0c72e8a16209",
   "metadata": {},
   "source": [
    "**Implementiion 6.6: Conjugate Gradient Method**\n",
    "\n",
    "We implement the CG method using `numpy` functions for matrix-vector and scalar products. We also make sure that we compute each product only once and reuse the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cg(A, b, x, k=100, tol=1e-5):\n",
    "    x = x.copy()\n",
    "    d, Ad, r = [np.zeros_like(r) for _ in range(3)]\n",
    "    r[:] = b - A.dot(x)\n",
    "    nrm_r2 = r.dot(r)\n",
    "    tol = tol**2\n",
    "\n",
    "    for i in range(k):\n",
    "        if nrm_r2 < tol:\n",
    "            break\n",
    "        Ad[:] = A.dot(d)\n",
    "        dAd = d.dot(Ad)\n",
    "        alpha = nrm_r2 / dAd\n",
    "        x[:] += alpha * d\n",
    "        r[:] -= alpha * Ad\n",
    "        nrm_r2_new = r.dot(r)\n",
    "\n",
    "        beta = nrm_r2_new / nrm_r2\n",
    "        d[:] = r + beta * d\n",
    "        nrm_r2 = nrm_r2_new\n",
    "    else:\n",
    "        print(f'CG method did not converge after {k} iterations.')\n",
    "    return x, i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e141b19-090b-41e7-a280-93ce79c33bf0",
   "metadata": {},
   "source": [
    "**Example 6.9**\n",
    "\n",
    "To compare the CG method with the previous iterative methods, we consider the model matrix again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a094f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    m = i * 10\n",
    "    n = m**2\n",
    "    N = np.diag(np.ones(m - 1), 1) + np.diag(np.ones(m - 1), -1)\n",
    "    B = 4 * np.eye(m) - N\n",
    "    A = np.kron(np.eye(m), B) - np.kron(N, np.eye(m))\n",
    "    b = np.ones(n)\n",
    "    x0 = np.zeros(n)\n",
    "    \n",
    "    t = time.perf_counter()\n",
    "    x, m = cg(A, b, x0, it=int(1e6), tol=1e-6)\n",
    "    t = time.perf_counter() - t\n",
    "    \n",
    "    res = np.linalg.norm(b - np.dot(A, x))\n",
    "    print(f'CG Method: n={n:05d}, Steps={m:05d} Time={t:07.4f} sec, res={res:4.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3803c226",
   "metadata": {},
   "source": [
    "As our implementation of the CG method does not require us to access individual entries of matrices and vectors, the implementation is even more efficient than the SOR method with optimal relaxation parameter.\n",
    "\n",
    "## 6.3 Preconditioned CG method\n",
    "\n",
    "**CG Method with Jacobi Preconditioning**\n",
    "\n",
    "For an efficient implementation of the preconditioned CG method, we have to decide what preconditioner $P\\approx A^{-1}$ to use. The simplest choice if the Jacobi preconditioner $P=D^{-1}$, where $D$ is the diagonal part of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f385ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cg_pre_jacobi(A, b, x, it=100, tol=1e-5):\n",
    "    x = x.copy()\n",
    "    r = b.copy() - A.dot(x)\n",
    "    P_inv = 1 / np.diag(A)\n",
    "    p = P_inv * r.copy()\n",
    "    d = p.copy()\n",
    "    rp = r.dot(p)\n",
    "    tol = tol**2\n",
    "    \n",
    "    for i in range(1, it + 1):\n",
    "        if abs(rp) < tol:\n",
    "            break\n",
    "        Ad = A.dot(d)\n",
    "        alpha = rp / d.dot(Ad)\n",
    "        x[:] += alpha * d\n",
    "        r[:] -= alpha * Ad\n",
    "        p[:] = P_inv * r\n",
    "        rp2 = r.dot(p)\n",
    "        beta = rp2 / rp\n",
    "        d[:] = p + beta * d\n",
    "        rp = rp2\n",
    "    else:\n",
    "        print(f'The Jabobi preconditioned CG method did not converge after {i} iterations.')\n",
    "    return x, i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965c15c7-b4f5-4a31-b10a-50df71aa0418",
   "metadata": {},
   "source": [
    "#### Example 6.11\n",
    "\n",
    "Applied to the model matrix, the Jacobi preconditioned CG method yields the following results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebee7a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    m = i * 10\n",
    "    n = m**2\n",
    "    N = np.diag(np.ones(m - 1), 1) + np.diag(np.ones(m - 1), -1)\n",
    "    B = 4 * np.eye(m) - N\n",
    "    A = np.kron(np.eye(m), B) - np.kron(N, np.eye(m))\n",
    "    b = np.ones(n)\n",
    "    x0 = np.zeros(n)\n",
    "    \n",
    "    t = time.perf_counter()\n",
    "    x, m = cg_pre_jacobi(A, b, x0, it=int(1e6), tol=1e-6)\n",
    "    t = time.perf_counter() - t\n",
    "    \n",
    "    res = np.linalg.norm(b - np.dot(A, x))\n",
    "    print(f'Jacobi preconditioned CG method: n={n:05d}, Steps={m:05d} Time={t:07.4f}sec, res={res:4.2e}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45cae50-2725-4ab5-ad05-62d29f584ccd",
   "metadata": {},
   "source": [
    "Jacobi preconditioning has not significantly accelerated the method. To implement the SSOR preconditioner, we implement the method with a general matrix $P$ and hide the application of  $P^{-1}$ in `np.linalg.solve(P, r)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a4d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cg_pre(A, P, b, x, it=100, tol=1e-5):\n",
    "    x = x.copy()\n",
    "    r = b.copy() - A.dot(x)\n",
    "    p = np.linalg.solve(P, r)\n",
    "    d = p.copy()\n",
    "    rp = r.dot(p)\n",
    "    tol = tol**2\n",
    "    \n",
    "    for i in range(1, it + 1):\n",
    "        if abs(rp) < tol:\n",
    "            break\n",
    "        Ad = A.dot(d)\n",
    "        alpha = rp / d.dot(Ad)\n",
    "        x[:] += alpha * d\n",
    "        r[:] -= alpha * Ad \n",
    "        p[:] = np.linalg.solve(P, r)\n",
    "        rp2 = r.dot(p)\n",
    "\n",
    "        beta = rp2 / rp\n",
    "        d[:] = p + beta * d\n",
    "        rp = rp2\n",
    "    else:\n",
    "        print(f'The precondioned CG method did not converge after {i} iterations.')\n",
    "    return x, i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f8a13-f0a0-420f-b651-2e8711ec1a4a",
   "metadata": {},
   "source": [
    "Applied to the model matrix and the optimal choice of $\\omega=2 - \\frac{2\\pi}{\\sqrt{n}}$, we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba1b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9):\n",
    "    m = i * 10\n",
    "    n = m**2\n",
    "    N = np.diag(np.ones(m - 1), 1) + np.diag(np.ones(m - 1), -1)\n",
    "    B = 4 * np.eye(m) - N\n",
    "    A = np.kron(np.eye(m), B) - np.kron(N, np.eye(m))\n",
    "    b = np.ones(n)\n",
    "    x0 = np.zeros(n)\n",
    "    b = np.ones(n)\n",
    "    x0 = np.zeros(n)\n",
    "    \n",
    "    omega = 2 - 2 * np.pi / np.sqrt(n)\n",
    "    D = np.diag(np.diag(A))\n",
    "    D1 = np.diag(1 / np.diag(A))\n",
    "    L = np.tril(A, -1)\n",
    "    R = np.triu(A, 1)\n",
    "    P = (D + omega * L) @ D1 @ (D + omega * R)\n",
    "    \n",
    "    t = time.perf_counter()\n",
    "    x, m = cg_pre(A, P, b, x0, it=int(1e6), tol=1e-6)\n",
    "    t = time.perf_counter() - t\n",
    "    \n",
    "    res = np.linalg.norm(b - np.dot(A, x))\n",
    "    print(f'SSOR-preconditioned CG-Verfahren: n={n:04d}, Steps={m:05d} Time={t:07.4f}sec, res={res:4.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac3831-fffd-4a2b-82f1-5b4695fb9b74",
   "metadata": {},
   "source": [
    "The convergence of the scheme has significantly improved and the number of necessary steps increases very slowly. However, solving the linear system in `p[:] = np.linalg.solve(P, r)` is much more expensive, so that each step takes significantly longer. This underlines the fact that a key property of an effective preconditioner, is that the application should be cheap."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
