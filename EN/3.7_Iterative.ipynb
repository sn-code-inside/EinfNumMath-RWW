{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ff045b",
   "metadata": {},
   "source": [
    "## 3.7 Iterative Methods for Solving Linear Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f33b8bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af309056",
   "metadata": {},
   "source": [
    "**The simple Richardson iteration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb0d4ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def richardson(A, b, x, it=1000, omega=1, tol=1e-5):\n",
    "    x = x.copy()\n",
    "    for i in range(it):\n",
    "        w = b - np.dot(A, x)\n",
    "        if np.linalg.norm(w) < tol:\n",
    "            break\n",
    "        x += omega * w\n",
    "    return x, i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06a0549-e5fa-4e20-92d6-4677a6e59b9d",
   "metadata": {},
   "source": [
    "We test the simple Richardson iterration with the linear system $Ax=b$ using\n",
    "$$A=\\begin{pmatrix}3 & 1.8 & 1\\\\ 1.4 & 2.3 & -0.7\\\\ 0.8 & 0.3 & 1.5 \\end{pmatrix}\\qquad\n",
    "b = \\begin{pmatrix} 1.2\\\\-2.1\\\\0.6\\end{pmatrix}.$$\n",
    "Using `numpy`, we get the following 'exact' solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ed2ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[3.0, 1.8, 1],\n",
    "              [1.4, 2.3, -0.7],\n",
    "              [0.8, 0.3, 1.5]])\n",
    "b = np.array([1.2, -2.1, 0.6])\n",
    "\n",
    "x_np = np.linalg.solve(A, b)\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdca537-fa06-42ea-9675-c696fbf6ce45",
   "metadata": {},
   "source": [
    "We apply the Richardson iteration to this system with the relaxation parameter $\\omega=1$ and the initial guess $x_0 = (1, -1, 0)$, which is reasonably close to the exact solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de1154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1.0, -1.0, 0.0])\n",
    "x, n = richardson(A, b, x0, it=100, omega=1)\n",
    "print(f'x = {x} after {n} steps')\n",
    "print(f'||x - x_ex||_2 = {np.linalg.norm(x - x_np)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68417be3-ad5e-4236-a0e7-bfe7e2e51680",
   "metadata": {},
   "source": [
    "The method clearly diverges. However, if we take a smaller relaxation parameter $\\omega$, we can achieve convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9756d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, n = richardson(A, b, x0, it=50, omega=0.4)\n",
    "print(f'x = {x} after {n} steps')\n",
    "print(f'||x - x_ex||_2 = {np.linalg.norm(x - x_np)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca779a",
   "metadata": {},
   "source": [
    "**Jacobi method**\n",
    "\n",
    "This implementation using `numpy` is **NOT OPTIMAL**, as we use Python loops to compute inner products, rather than using functionality provided by `numpy`. We opted for this approach to highlight the differences between the Jacobi and Gauss-Seidel method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d80f2e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobi(A, b, x, it=1000, tol=1e-5):\n",
    "    n, m = A.shape\n",
    "    x, x_neu = x.copy(), x.copy()\n",
    "    for k in range(it):\n",
    "        if np.linalg.norm(b - np.dot(A, x)) < tol:\n",
    "            break\n",
    "        for i in range(n):\n",
    "            s = 0\n",
    "            for j in range(i):\n",
    "                s += A[i, j] * x[j]\n",
    "            for j in range(i + 1, n):\n",
    "                s += A[i, j] * x[j]\n",
    "            x_neu[i] = (b[i] - s) / A[i, i]\n",
    "        x[:] = x_neu\n",
    "    return x, k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5353983c-5d06-4c12-95cf-7293af6778f6",
   "metadata": {},
   "source": [
    "Applying this to our previous example yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eee4bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, n = jacobi(A, b, x0, it=50)\n",
    "print(f'x = {x} after {n} steps')\n",
    "print(f'||x - x_ex||_2 = {np.linalg.norm(x - x_np)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b2700b",
   "metadata": {},
   "source": [
    "**Gauss-Seidel method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3582128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_seidel(A, b, x, it=100, tol=1e-5):\n",
    "    n, m = A.shape\n",
    "    x, x_neu = x.copy(), x.copy()\n",
    "    for k in range(it):\n",
    "        if np.linalg.norm(b - np.dot(A, x)) < tol:\n",
    "            break\n",
    "        for i in range(n):\n",
    "            s = 0\n",
    "            for j in range(i):\n",
    "                s += A[i, j] * x_neu[j]\n",
    "            for j in range(i + 1, n):\n",
    "                s += A[i, j] * x[j]\n",
    "            x_neu[i] = (b[i] - s) / A[i, i]\n",
    "        x[:] = x_neu\n",
    "    return x, k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb8ce33-04ae-4d0f-a268-3db526f91274",
   "metadata": {},
   "source": [
    "Applying this to our previous example yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05fe6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, n = gauss_seidel(A, b, x0, it=50)\n",
    "print(f'x = {x} after {n} steps')\n",
    "print(f'||x - x_ex||_2 = {np.linalg.norm(x - x_np)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3b1e11-51c4-44cf-9d9e-a5568e682b2a",
   "metadata": {},
   "source": [
    "We see that the Gauss-Seidel method requires only about half the number of steps to achive the same accuracy.\n",
    "\n",
    "The Gauss-Seidel method requires about half of the iteration steps than the Jacobi method.\n",
    "\n",
    "### Convergence criteria for the Jacobi and Gauss-Seidel methods\n",
    "\n",
    "#### Example 3.47 (Jacobi and Gauss-Seidel methods for the model matrix)\n",
    "\n",
    "We consider the linear system $Ax=b$ with the matrix $A\\in\\mathbb{R}^{n\\times n}$ given by\n",
    "$$ A = \\begin{pmatrix}2 & -1 \\\\ -1 & 2 & -1 \\\\ & \\ddots & \\ddots & \\ddots \\\\ && -1 & 2 & -1\\\\ &&& -1 & 2 \\end{pmatrix},$$\n",
    "and the right-hand side vector $b\\in\\mathbb{R}^n$ mit $b=(1,\\dots,1)^T$. The matrix is irreducible, diagonally dominant while the first andf last rows are stricly diagnally dominant. Therefore, both the Jacobi and Gauss-Seidel methods converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "083ef586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f46ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    n = 10 * 2**i\n",
    "    A = np.diag(2 * np.ones(n), k=0) + np.diag(-1 * np.ones(n - 1), k=1) + np.diag(-1 * np.ones(n - 1), k=-1)\n",
    "    b = np.ones(n)\n",
    "    x0 = np.zeros(n)\n",
    "    \n",
    "    t = time.perf_counter()\n",
    "    x, m = jacobi(A, b, x0, it=int(2e6), tol=1e-4)\n",
    "    t = time.perf_counter() - t\n",
    "    \n",
    "    res = np.linalg.norm(b - np.dot(A, x))\n",
    "    print(f'Jacobi: n = {n:03d}, steps = {m:07d} time = {t:07.3f}sec, res = {res:4.2e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e956fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    n = 10 * 2**i\n",
    "    A = np.diag(2 * np.ones(n), k=0) + np.diag(-1 * np.ones(n - 1), k=1) + np.diag(-1 * np.ones(n - 1), k=-1)\n",
    "    b = np.ones(n)\n",
    "    x0 = np.zeros(n)\n",
    "    \n",
    "    t = time.perf_counter()\n",
    "    x, m = gauss_seidel(A, b, x0, it=int(2e6), tol=1e-4)\n",
    "    t = time.perf_counter() - t\n",
    "    \n",
    "    res = np.linalg.norm(b - np.dot(A, x))\n",
    "    print(f'Gauss-Seidel: n = {n:03d}, steps = {m:07d} time = {t:07.3f}sec, res = {res:4.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c3b5d4",
   "metadata": {},
   "source": [
    "This shows that the Gauss-Seidel methods needs almost exactly half as many steps as the Jacobi method. Consequently, it is twice as fast if the methods are implemented equally efficiently.\n",
    "\n",
    "Using `numpy`s functionality to compute inner products, we can implement the methods significantly more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59ccd190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobi_np(A, b, x, it=1000, tol=1e-5):\n",
    "    n, m = A.shape\n",
    "    d = np.diag(A)\n",
    "    x = x.copy()\n",
    "    for k in range(it):\n",
    "        res = b - A.dot(x)\n",
    "        if np.linalg.norm(res) < tol:\n",
    "            break\n",
    "        x += res / d\n",
    "    return x, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51a0b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_seidel_np(A, b, x, it=100, tol=1e-5):\n",
    "    n, m = A.shape\n",
    "    x = x.copy()\n",
    "    for k in range(it):\n",
    "        if np.linalg.norm(b - np.dot(A, x)) < tol:\n",
    "            break\n",
    "        x_alt = x.copy()\n",
    "        for i in range(n):\n",
    "            s1 = np.dot(A[i, :i], x[:i])\n",
    "            s2 = np.dot(A[i, i + 1:], x_alt[i + 1:])\n",
    "            x[i] = (b[i] - s1 - s2) / A[i, i]\n",
    "    return x, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b09023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    n = 10 * 2**i\n",
    "    A = np.diag(2 * np.ones(n), k=0) + np.diag(-1 * np.ones(n - 1), k=1) + np.diag(-1 * np.ones(n - 1), k=-1)\n",
    "    b = np.ones(n)\n",
    "    x0 = np.zeros(n)\n",
    "    \n",
    "    t = time.perf_counter()\n",
    "    x, m = jacobi_np(A, b, x0, it=int(2e6), tol=1e-4)\n",
    "    t = time.perf_counter() - t\n",
    "    \n",
    "    res = np.linalg.norm(b - np.dot(A, x))\n",
    "    print(f'Jacobi: n = {n:03d}, steps = {m:07d} time = {t:07.3f}sec, res = {res:4.2e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4bb2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    n = 10 * 2**i\n",
    "    A = np.diag(2 * np.ones(n), k=0) + np.diag(-1 * np.ones(n - 1), k=1) + np.diag(-1 * np.ones(n - 1), k=-1)\n",
    "    b = np.ones(n)\n",
    "    x0 = np.zeros(n)\n",
    "    \n",
    "    t = time.perf_counter()\n",
    "    x, m = gauss_seidel_np(A, b, x0, it=int(1e6), tol=1e-4)\n",
    "    t = time.perf_counter() - t\n",
    "    \n",
    "    res = np.linalg.norm(b - np.dot(A, x))\n",
    "    print(f'Gauss-Seidel: n = {n:03d}, steps = {m:07d} time = {t:07.3f}sec, res = {res:4.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb640dc",
   "metadata": {},
   "source": [
    "Here, we see that accessing individual entries of our vector and matrix from python is significantly more expensive than the computation of inner products by `numpy`. This results in the Jacobi method beating the Gauss-Seidel method, even though the latter needs half as many iterations. However, by using the matrix form of the GauÃŸ-Seidel method, and using an optimized `scipy` routine for the forward solve step, we get a similar level of efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb789624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "def gauss_seidel_sp(A, b, x, it=100, tol=1e-5):\n",
    "    n, m = A.shape\n",
    "    x = x.copy()\n",
    "    R = np.triu(A, 1)\n",
    "    LD = np.tril(A, 0)\n",
    "    for k in range(it):\n",
    "        if np.linalg.norm(b - np.dot(A, x)) < tol:\n",
    "            break\n",
    "        x = sp.linalg.solve_triangular(LD, b - np.dot(R, x), lower=True)\n",
    "    return x, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b2854d-1423-4779-a641-e56cd739ec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    n = 10 * 2**i\n",
    "    A = np.diag(2 * np.ones(n), k=0) + np.diag(-1 * np.ones(n - 1), k=1) + np.diag(-1 * np.ones(n - 1), k=-1)\n",
    "    b = np.ones(n)\n",
    "    x0 = np.zeros(n)\n",
    "    \n",
    "    t = time.perf_counter()\n",
    "    x, m = gauss_seidel_sp(A, b, x0, it=int(1e6), tol=1e-4)\n",
    "    t = time.perf_counter() - t\n",
    "    \n",
    "    res = np.linalg.norm(b - np.dot(A, x))\n",
    "    print(f'Gauss-Seidel: n = {n:03d}, steps = {m:07d} time = {t:07.3f}sec, res = {res:4.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a1c5d6-1633-4a2c-8e9e-ba06401a9a94",
   "metadata": {},
   "source": [
    "This highlights that an efficient implementation of an algorithm is key to its performance in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e76ae1-c9eb-4104-b7e1-586cdc1adf78",
   "metadata": {},
   "source": [
    "### 3.7.2 Relaxation methods: The SOR method\n",
    "\n",
    "We implement the SOR method in the same manner as we implemented the Jacobi and Gauss-Seidel methods, i.e., with loops in python and direct access to individual entries. Consequently, we can compare the methods' efficiency against these implementations of the latter methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ddbc250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sor(A, b, x, omega, it=100, tol=1e-5):\n",
    "    assert (omega > 0 and omega < 2), 'omega not contained in (0, 2)'\n",
    "    n, m = A.shape\n",
    "    x, x_neu = x.copy(), x.copy()\n",
    "    for k in range(it):\n",
    "        if np.linalg.norm(b - np.dot(A, x)) < tol:\n",
    "            break\n",
    "        for i in range(n):\n",
    "            s = 0\n",
    "            for j in range(i):\n",
    "                s += A[i, j] * x_neu[j]\n",
    "            for j in range(i + 1, n):\n",
    "                s += A[i, j] * x[j]\n",
    "            x_neu[i] = omega * (b[i] - s) / A[i, i] + (1 - omega) * x[i]\n",
    "        x[:] = x_neu\n",
    "    return x, k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2dfac7-c83f-429c-b392-cdfa7d2952dc",
   "metadata": {},
   "source": [
    "We first consider again the system $Ax=b$ with $$A=\\begin{pmatrix}3 & 1.8 & 1\\\\ 1.4 & 2.3 & -0.7\\\\ 0.8 & 0.3 & 1.5 \\end{pmatrix}\\qquad b = \\begin{pmatrix} 1.2\\\\-2.1\\\\0.6\\end{pmatrix}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e52bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[3.0, 1.8, 1],\n",
    "              [1.4, 2.3, -0.7],\n",
    "              [0.8, 0.3, 1.5]])\n",
    "b = np.array([1.2, -2.1, 0.6])\n",
    "\n",
    "x_np = np.linalg.solve(A, b)\n",
    "\n",
    "x0 = np.array([1.0, -1.0, 0.0])\n",
    "x, n = sor(A, b, x0, it=100, omega=1.2)\n",
    "print(f'x = {x} after {n} steps')\n",
    "print(f'||x - x_ex||_2 = {np.linalg.norm(x - x_np)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f10b25-f943-4079-a5ec-4ed1891f39d8",
   "metadata": {},
   "source": [
    "The method converges faster than the previous two methods. However, this strongly depends on the correct choice of $\\omega$. Try different values for $\\omega$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9cd96c-95f9-479a-96e3-e87a0cdfc2b7",
   "metadata": {},
   "source": [
    "#### Example 3.49 (Model matrix system with the SOR method.)\n",
    "\n",
    "We return to the model matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ced6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    n = 10 * 2**i\n",
    "    A = np.diag(2 * np.ones(n), k=0) + np.diag(-1 * np.ones(n - 1), k=1) + np.diag(-1 * np.ones(n - 1), k=-1)\n",
    "    b = np.ones(n)\n",
    "    x0 = np.zeros(n)\n",
    "    \n",
    "    lam = 1 - np.pi**2 / (2 * (n+1)**2)\n",
    "    omega = 2 * (1 - np.sqrt(1 - lam**2)) / lam**2\n",
    "    \n",
    "    t = time.perf_counter()\n",
    "    x, m = sor(A, b, x0, omega=omega, it=int(2e6), tol=1e-4)\n",
    "    t = time.perf_counter() - t\n",
    "    \n",
    "    res = np.linalg.norm(b - np.dot(A, x))\n",
    "    print(f'SOR: n = {n:03d}, steps = {m:07d} time = {t:07.3f}sec, res = {res:4.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73866fea",
   "metadata": {},
   "source": [
    "For $n=80$, the SOR method is about 40-times faster than the Gauss-Seidel method. \n",
    "\n",
    "Using `numpy`, we can again improve the performance a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b060f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sor_np(A, b, x, omega, it=100, tol=1e-5):\n",
    "    n, m = A.shape\n",
    "    x, x_alt = x.copy(), x.copy()\n",
    "    for k in range(it):\n",
    "        if np.linalg.norm(b - np.dot(A, x)) < tol:\n",
    "            break\n",
    "        x_alt[:] = x\n",
    "        for i in range(n):\n",
    "            s1 = np.dot(A[i, :i], x[:i])\n",
    "            s2 = np.dot(A[i, i + 1:], x_alt[i + 1:])\n",
    "            x[i] = omega * (b[i] - s1 - s2) / A[i, i] + (1 - omega) * x_alt[i]\n",
    "    return x, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    n = 10 * 2**i\n",
    "    A = np.diag(2 * np.ones(n), k=0) + np.diag(-1 * np.ones(n - 1), k=1) + np.diag(-1 * np.ones(n - 1), k=-1)\n",
    "    b = np.ones(n)\n",
    "    x0 = np.zeros(n)\n",
    "    \n",
    "    lam = 1 - np.pi**2 / (2 * (n+1)**2)\n",
    "    omega = 2 * (1 - np.sqrt(1 - lam**2)) / lam**2\n",
    "    \n",
    "    t = time.perf_counter()\n",
    "    x, m = sor_np(A, b, x0, omega=omega, it=int(2e6), tol=1e-4)\n",
    "    t = time.perf_counter() - t\n",
    "    \n",
    "    res = np.linalg.norm(b - np.dot(A, x))\n",
    "    print(f'SOR: n = {n:03d}, steps = {m:07d} time = {t:07.3f}sec, res = {res:4.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5a171c",
   "metadata": {},
   "source": [
    "Due to the significant reduction in the number of necessary iteration steps, the SOR method is faster than the Jacobi method using only `numpy` inner products, even though we have to access parts of the matrix and vectors for the SOR method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
